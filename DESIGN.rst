**Summary**: Data Profiling using pyspark is a project desinged to ease and automate the data profiling task.

.. contents::
   :depth: 2
   :backlinks: top
   :local:

Quick Start
-----------

* Clone the project (or download a zip archive)
```bash
git clone git@github.com:erfankashani/spark_profiling_package.git
```

* Install Dependencies
```bash
pip install -r requirements
```

* Optional: Install the package for dev mode
```bash
pip install -e .
```

Contacts
--------

* Lead developer: Erfan K (https://github.com/erfankashani/python_project_template)
* Development manager: 
* QA: 
* Operations contact: 
* Architect: Erfan K


Timelines
---------

Example:
* April, 2022: Inital package creation from legecy notebooks
* January, 2023: refactoring

Monitoring
----------

* Production monitoring link
* Production log reports link
* Development reports link


Specifications
--------------


Environment
-----------


Dependencies
------------

Main Dependencies is Spark instance. Developer should have spark settup or use a cloud environment where it is available

Services
~~~~~~~~

**Profiler**: Performs the profiling



Libraries
~~~~~~~~~

project is tested on Python 3.8.

Architecture
------------


Testing strategy
----------------

(Example)
* Unit tests are in the /test/ directory, run with py.test


Tickets
-------

Important tickets of record.

(Example)
* Provision the pool
* Schema review
* Security architecture review
* Network topology setup
* Firewall exceptions
* Firewall exceptions #2 for real this time

